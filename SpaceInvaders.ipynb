{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done: break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart and Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "  \"\"\"Random Agent that will play the specified game\n",
    "    Arguments:\n",
    "      env_name: Name of the environment to be played\n",
    "      max_eps: Maximum number of episodes to run agent for.\n",
    "  \"\"\"\n",
    "  def __init__(self, env_name, max_eps):\n",
    "    self.env = gym.make(env_name)\n",
    "    self.max_episodes = max_eps\n",
    "    self.global_moving_average_reward = 0\n",
    "    self.res_queue = Queue()\n",
    "\n",
    "  def run(self):\n",
    "    reward_avg = 0\n",
    "    for episode in range(self.max_episodes):\n",
    "      done = False\n",
    "      self.env.reset()\n",
    "      reward_sum = 0.0\n",
    "      steps = 0\n",
    "      while not done:\n",
    "        # Sample randomly from the action space and step\n",
    "        _, reward, done, _ = self.env.step(self.env.action_space.sample())\n",
    "        steps += 1\n",
    "        reward_sum += reward\n",
    "      # Record statistics\n",
    "      self.global_moving_average_reward = record(episode, \n",
    "                                                 reward_sum, \n",
    "                                                 0,\n",
    "                                                 self.global_moving_average_reward,\n",
    "                                                 self.res_queue, 0, steps)\n",
    "\n",
    "      reward_avg += reward_sum\n",
    "    final_avg = reward_avg / float(self.max_episodes)\n",
    "    print(\"Average score across {} episodes: {}\".format(self.max_episodes, final_avg))\n",
    "    return final_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticModel(keras.Model):\n",
    "  def __init__(self, state_size, action_size):\n",
    "    super(ActorCriticModel, self).__init__()\n",
    "    self.state_size = state_size\n",
    "    self.action_size = action_size\n",
    "    self.dense1 = layers.Dense(100, activation='relu')\n",
    "    self.policy_logits = layers.Dense(action_size)\n",
    "    self.dense2 = layers.Dense(100, activation='relu')\n",
    "    self.values = layers.Dense(1)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Forward pass\n",
    "    x = self.dense1(inputs)\n",
    "    logits = self.policy_logits(x)\n",
    "    v1 = self.dense2(inputs)\n",
    "    values = self.values(v1)\n",
    "    return logits, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterAgent():\n",
    "  def __init__(self):\n",
    "    self.game_name = 'CartPole-v0'\n",
    "    save_dir = args.save_dir\n",
    "    self.save_dir = save_dir\n",
    "    if not os.path.exists(save_dir):\n",
    "      os.makedirs(save_dir)\n",
    "\n",
    "    env = gym.make(self.game_name)\n",
    "    self.state_size = env.observation_space.shape[0]\n",
    "    self.action_size = env.action_space.n\n",
    "    self.opt = tf.train.AdamOptimizer(args.lr, use_locking=True)\n",
    "    print(self.state_size, self.action_size)\n",
    "\n",
    "    self.global_model = ActorCriticModel(self.state_size, self.action_size)  # global network\n",
    "    self.global_model(tf.convert_to_tensor(np.random.random((1, self.state_size)), dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "    if args.algorithm == 'random':\n",
    "      random_agent = RandomAgent(self.game_name, args.max_eps)\n",
    "      random_agent.run()\n",
    "      return\n",
    "\n",
    "    res_queue = Queue()\n",
    "\n",
    "    workers = [Worker(self.state_size,\n",
    "                      self.action_size,\n",
    "                      self.global_model,\n",
    "                      self.opt, res_queue,\n",
    "                      i, game_name=self.game_name,\n",
    "                      save_dir=self.save_dir) for i in range(multiprocessing.cpu_count())]\n",
    "\n",
    "    for i, worker in enumerate(workers):\n",
    "      print(\"Starting worker {}\".format(i))\n",
    "      worker.start()\n",
    "\n",
    "    moving_average_rewards = []  # record episode reward to plot\n",
    "    while True:\n",
    "      reward = res_queue.get()\n",
    "      if reward is not None:\n",
    "        moving_average_rewards.append(reward)\n",
    "      else:\n",
    "        break\n",
    "    [w.join() for w in workers]\n",
    "\n",
    "    plt.plot(moving_average_rewards)\n",
    "    plt.ylabel('Moving average ep reward')\n",
    "    plt.xlabel('Step')\n",
    "    plt.savefig(os.path.join(self.save_dir,\n",
    "                             '{} Moving Average.png'.format(self.game_name)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "  def __init__(self):\n",
    "    self.states = []\n",
    "    self.actions = []\n",
    "    self.rewards = []\n",
    "    \n",
    "  def store(self, state, action, reward):\n",
    "    self.states.append(state)\n",
    "    self.actions.append(action)\n",
    "    self.rewards.append(reward)\n",
    "    \n",
    "  def clear(self):\n",
    "    self.states = []\n",
    "    self.actions = []\n",
    "    self.rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(threading.Thread):\n",
    "  # Set up global variables across different threads\n",
    "  global_episode = 0\n",
    "  # Moving average reward\n",
    "  global_moving_average_reward = 0\n",
    "  best_score = 0\n",
    "  save_lock = threading.Lock()\n",
    "\n",
    "  def __init__(self,\n",
    "               state_size,\n",
    "               action_size,\n",
    "               global_model,\n",
    "               opt,\n",
    "               result_queue,\n",
    "               idx,\n",
    "               game_name='CartPole-v0',\n",
    "               save_dir='/tmp'):\n",
    "    super(Worker, self).__init__()\n",
    "    self.state_size = state_size\n",
    "    self.action_size = action_size\n",
    "    self.result_queue = result_queue\n",
    "    self.global_model = global_model\n",
    "    self.opt = opt\n",
    "    self.local_model = ActorCriticModel(self.state_size, self.action_size)\n",
    "    self.worker_idx = idx\n",
    "    self.game_name = game_name\n",
    "    self.env = gym.make(self.game_name).unwrapped\n",
    "    self.save_dir = save_dir\n",
    "    self.ep_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self):\n",
    "    total_step = 1\n",
    "    mem = Memory()\n",
    "    while Worker.global_episode < args.max_eps:\n",
    "      current_state = self.env.reset()\n",
    "      mem.clear()\n",
    "      ep_reward = 0.\n",
    "      ep_steps = 0\n",
    "      self.ep_loss = 0\n",
    "\n",
    "      time_count = 0\n",
    "      done = False\n",
    "      while not done:\n",
    "        logits, _ = self.local_model(\n",
    "            tf.convert_to_tensor(current_state[None, :],\n",
    "                                 dtype=tf.float32))\n",
    "        probs = tf.nn.softmax(logits)\n",
    "\n",
    "        action = np.random.choice(self.action_size, p=probs.numpy()[0])\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "        if done:\n",
    "          reward = -1\n",
    "        ep_reward += reward\n",
    "        mem.store(current_state, action, reward)\n",
    "\n",
    "        if time_count == args.update_freq or done:\n",
    "          # Calculate gradient wrt to local model. We do so by tracking the\n",
    "          # variables involved in computing the loss by using tf.GradientTape\n",
    "          with tf.GradientTape() as tape:\n",
    "            total_loss = self.compute_loss(done,\n",
    "                                           new_state,\n",
    "                                           mem,\n",
    "                                           args.gamma)\n",
    "          self.ep_loss += total_loss\n",
    "          # Calculate local gradients\n",
    "          grads = tape.gradient(total_loss, self.local_model.trainable_weights)\n",
    "          # Push local gradients to global model\n",
    "          self.opt.apply_gradients(zip(grads,\n",
    "                                       self.global_model.trainable_weights))\n",
    "          # Update local model with new weights\n",
    "          self.local_model.set_weights(self.global_model.get_weights())\n",
    "\n",
    "          mem.clear()\n",
    "          time_count = 0\n",
    "\n",
    "          if done:  # done and print information\n",
    "            Worker.global_moving_average_reward = \\\n",
    "              record(Worker.global_episode, ep_reward, self.worker_idx,\n",
    "                     Worker.global_moving_average_reward, self.result_queue,\n",
    "                     self.ep_loss, ep_steps)\n",
    "            # We must use a lock to save our model and to print to prevent data races.\n",
    "            if ep_reward > Worker.best_score:\n",
    "              with Worker.save_lock:\n",
    "                print(\"Saving best model to {}, \"\n",
    "                      \"episode score: {}\".format(self.save_dir, ep_reward))\n",
    "                self.global_model.save_weights(\n",
    "                    os.path.join(self.save_dir,\n",
    "                                 'model_{}.h5'.format(self.game_name))\n",
    "                )\n",
    "                Worker.best_score = ep_reward\n",
    "            Worker.global_episode += 1\n",
    "        ep_steps += 1\n",
    "\n",
    "        time_count += 1\n",
    "        current_state = new_state\n",
    "        total_step += 1\n",
    "    self.result_queue.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self,\n",
    "                   done,\n",
    "                   new_state,\n",
    "                   memory,\n",
    "                   gamma=0.99):\n",
    "    if done:\n",
    "      reward_sum = 0.  # terminal\n",
    "    else:\n",
    "      reward_sum = self.local_model(\n",
    "          tf.convert_to_tensor(new_state[None, :],\n",
    "                               dtype=tf.float32))[-1].numpy()[0]\n",
    "\n",
    "    # Get discounted rewards\n",
    "    discounted_rewards = []\n",
    "    for reward in memory.rewards[::-1]:  # reverse buffer r\n",
    "      reward_sum = reward + gamma * reward_sum\n",
    "      discounted_rewards.append(reward_sum)\n",
    "    discounted_rewards.reverse()\n",
    "\n",
    "    logits, values = self.local_model(\n",
    "        tf.convert_to_tensor(np.vstack(memory.states),\n",
    "                             dtype=tf.float32))\n",
    "    # Get our advantages\n",
    "    advantage = tf.convert_to_tensor(np.array(discounted_rewards)[:, None],\n",
    "                            dtype=tf.float32) - values\n",
    "    # Value loss\n",
    "    value_loss = advantage ** 2\n",
    "\n",
    "    # Calculate our policy loss\n",
    "    actions_one_hot = tf.one_hot(memory.actions, self.action_size, dtype=tf.float32)\n",
    "\n",
    "    policy = tf.nn.softmax(logits)\n",
    "    entropy = tf.reduce_sum(policy * tf.log(policy + 1e-20), axis=1)\n",
    "\n",
    "    policy_loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=actions_one_hot,\n",
    "                                                             logits=logits)\n",
    "    policy_loss *= tf.stop_gradient(advantage)\n",
    "    policy_loss -= 0.01 * entropy\n",
    "    total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(self):\n",
    "    env = gym.make(self.game_name).unwrapped\n",
    "    state = env.reset()\n",
    "    model = self.global_model\n",
    "    model_path = os.path.join(self.save_dir, 'model_{}.h5'.format(self.game_name))\n",
    "    print('Loading model from: {}'.format(model_path))\n",
    "    model.load_weights(model_path)\n",
    "    done = False\n",
    "    step_counter = 0\n",
    "    reward_sum = 0\n",
    "\n",
    "    try:\n",
    "      while not done:\n",
    "        env.render(mode='rgb_array')\n",
    "        policy, value = model(tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n",
    "        policy = tf.nn.softmax(policy)\n",
    "        action = np.argmax(policy)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        print(\"{}. Reward: {}, action: {}\".format(step_counter, reward_sum, action))\n",
    "        step_counter += 1\n",
    "    except KeyboardInterrupt:\n",
    "      print(\"Received Keyboard Interrupt. Shutting down.\")\n",
    "    finally:\n",
    "      env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
